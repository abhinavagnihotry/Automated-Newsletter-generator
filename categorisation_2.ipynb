{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce46701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"filtered_llmops_database.csv\")\n",
    "df.head()\n",
    "df = df.sample(2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fa09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas pyarrow openai>=1.40.0\n",
    "import os, json, time, uuid, pathlib\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "MODEL = \"gpt-4o-mini\"      # good price/quality for routing\n",
    "BATCH_WINDOW = \"24h\"       # required by Batch API\n",
    "INPUT_JSONL = \"news_taxonomy_requests.jsonl\"\n",
    "OUTPUT_JSONL = \"news_taxonomy_results.jsonl\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Set OPENAI_API_KEY in your environment.\")\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d0d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Taxonomy (your catalog)\n",
    "# ---------------------------\n",
    "CATALOG = {\n",
    "    \"categories\": [\n",
    "        {\n",
    "            \"key\": \"research_highlights\",\n",
    "            \"name\": \"Research Highlights\",\n",
    "            \"definition\": \"New papers, preprints, benchmarks, datasets, SOTA claims, academic results.\",\n",
    "            \"example_signals\": [\"arXiv\", \"preprint\", \"benchmark\", \"dataset\", \"NeurIPS\", \"ICLR\", \"Nature\", \"SOTA\"],\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"industry_news\",\n",
    "            \"name\": \"Industry News\",\n",
    "            \"definition\": \"Funding, acquisitions, partnerships, regs/policy, leadership changes, earnings, market moves.\",\n",
    "            \"example_signals\": [\"raises\", \"acquired\", \"partnership\", \"announces partnership\", \"regulation\", \"EU AI Act\", \"SEC filing\"],\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"cool_use_cases\",\n",
    "            \"name\": \"Cool Use Cases\",\n",
    "            \"definition\": \"Real deployments, case studies, measurable impact, pilots, customer rollouts.\",\n",
    "            \"example_signals\": [\"in production\", \"case study\", \"pilot\", \"rollout\", \"customers\", \"ROI\", \"impact\"],\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"engineering_deep_dives\",\n",
    "            \"name\": \"Engineering Deep Dives\",\n",
    "            \"definition\": \"Concrete system/infra write-ups on scaling, latency, reliability, inference, distillation, orchestration.\",\n",
    "            \"example_signals\": [\"scaling\", \"latency\", \"throughput\", \"GPU\", \"distillation\", \"load balancing\", \"traffic management\", \"observability\"],\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"product_launches_tools\",\n",
    "            \"name\": \"Product Launches & Tools\",\n",
    "            \"definition\": \"New products, features, SDKs, APIs, GA/preview, open-source releases.\",\n",
    "            \"example_signals\": [\"launch\", \"introducing\", \"announces\", \"SDK\", \"API\", \"general availability\", \"open source\", \"release notes\"],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Prompts\n",
    "# ---------------------------\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an editor routing tech news into newsletter sections. \"\n",
    "    \"Be decisive, concise, and consistent with the provided category definitions. \"\n",
    "    \"When unsure, still score all categories in [1, 5]. Favor the most specific category. \"\n",
    "    \"Return STRICT JSON that matches the JSON schema and nothing else.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d810f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_user_prompt(article: dict) -> str:\n",
    "    \"\"\"\n",
    "    article = {\n",
    "      title, summaries, tags, url, company, created_at\n",
    "    }\n",
    "    \"\"\"\n",
    "    schema = (\n",
    "        \"{\\n\"\n",
    "        '  \"primary_category\": \"string\",\\n'\n",
    "        '  \"secondary_categories\": [\"string\", ...],\\n'\n",
    "        '  \"scores\": {\"category_key\": <score>, \"...\": <score>},\\n'\n",
    "        '  \"newsletter_score\": <score>\\n'\n",
    "        \"}\"\n",
    "    )\n",
    "    return (\n",
    "        \"You will receive:\\n\"\n",
    "        \"1) Article fields (title, summaries, tags, url, company, created_at).\\n\"\n",
    "        \"2) Category catalog (key, definition, example_signals).\\n\\n\"\n",
    "        \"Tasks:\\n\"\n",
    "        \"A) Score every category from 0 to 5 based on evidence in the article.\\n\"\n",
    "        \"B) Pick the best category. Provide up to 2 secondaries if close.\\n\"\n",
    "        \"JSON schema (return exactly this):\\n\"\n",
    "        + schema + \"\\n\\n\"\n",
    "        \"Article:\\n\" + json.dumps(article, ensure_ascii=False) + \"\\n\\n\"\n",
    "        \"Categories:\\n\" + json.dumps(CATALOG, ensure_ascii=False)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079b3a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2 tasks to news_taxonomy_requests.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Build batch requests JSONL\n",
    "# ---------------------------\n",
    "df = df.reset_index(drop=False).rename(columns={\"index\": \"_row_index\"})\n",
    "tasks = []\n",
    "\n",
    "def _split_tags(s):\n",
    "    if pd.isna(s) or not str(s).strip():\n",
    "        return []\n",
    "    return [t.strip() for t in str(s).split(\",\") if t.strip()]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Build the article payload the model will see\n",
    "    article = {\n",
    "        \"article\": row.get(\"full_summary\"),\n",
    "        \"tags\": list(dict.fromkeys(  # de-duplicate while preserving order\n",
    "            _split_tags(row.get(\"application_tags\", \"\")) +\n",
    "            _split_tags(row.get(\"tools_tags\", \"\")) +\n",
    "            _split_tags(row.get(\"techniques_tags\", \"\")) +\n",
    "            _split_tags(row.get(\"extra_tags\", \"\"))\n",
    "        )),\n",
    "        \"url\": row.get(\"source_url\"),\n",
    "        \"company\": row.get(\"company\"),\n",
    "        \"created_at\": row.get(\"created_at\"),\n",
    "    }\n",
    "\n",
    "    custom_id = f\"row-{row['_row_index']}\"\n",
    "    user_prompt = build_user_prompt(article)\n",
    "\n",
    "    # One line per request for the Batch API (Chat Completions flavor)\n",
    "    # Format per OpenAI cookbook: custom_id/method/url/body\n",
    "    # https://cookbook.openai.com/examples/batch_processing\n",
    "    task = {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": MODEL,\n",
    "            \"temperature\": 0.0,\n",
    "            \"response_format\": {\"type\": \"json_object\"},  # JSON Mode\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "    tasks.append(task)\n",
    "\n",
    "with open(INPUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in tasks:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(tasks)} tasks to {INPUT_JSONL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2500912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: batch_68c40c24abdc81908d43d204d5e75cea Status: validating\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Submit batch\n",
    "# ---------------------------\n",
    "batch_file = client.files.create(file=open(INPUT_JSONL, \"rb\"), purpose=\"batch\")\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=BATCH_WINDOW,\n",
    ")\n",
    "print(\"Batch ID:\", batch.id, \"Status:\", batch.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d5751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:33:56] status=in_progress\n",
      "[17:34:12] status=in_progress\n",
      "[17:34:27] status=in_progress\n",
      "[17:34:42] status=in_progress\n",
      "[17:34:58] status=in_progress\n",
      "[17:35:14] status=in_progress\n",
      "[17:35:30] status=completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: poll until done (you can also retrieve later by ID)\n",
    "def wait_for_batch(batch_id: str, poll_seconds: int = 15):\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch_id)\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] status={b.status}\")\n",
    "        if b.status in (\"completed\", \"failed\", \"expired\", \"cancelling\", \"cancelled\"):\n",
    "            return b\n",
    "        time.sleep(poll_seconds)\n",
    "\n",
    "batch = wait_for_batch(batch.id)\n",
    "if batch.status != \"completed\":\n",
    "    raise RuntimeError(f\"Batch ended with status={batch.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82593484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to news_taxonomy_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Download results JSONL\n",
    "# ---------------------------\n",
    "result_bytes = client.files.content(batch.output_file_id).content\n",
    "with open(OUTPUT_JSONL, \"wb\") as f:\n",
    "    f.write(result_bytes)\n",
    "print(f\"Saved results to {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ddaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Simple parser: read JSONL -> normalize -> merge\n",
    "# ---------------------------\n",
    "import pandas as pd, json, re\n",
    "\n",
    "def parse_batch_results(path: str):\n",
    "    raw = pd.read_json(path, lines=True)\n",
    "\n",
    "    def ok(resp):\n",
    "        try:\n",
    "            return resp.get(\"status_code\", 0) == 200\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    # Successful rows\n",
    "    ok_rows = raw[raw[\"response\"].apply(ok)].copy()\n",
    "\n",
    "    def to_parsed(resp):\n",
    "        try:\n",
    "            content = resp[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            return json.loads(content)\n",
    "        except Exception as e:\n",
    "            return {\"_error\": f\"parse_error: {e}\"}\n",
    "\n",
    "    ok_rows[\"parsed\"] = ok_rows[\"response\"].apply(to_parsed)\n",
    "    ok_rows[\"row_index\"] = (\n",
    "        ok_rows[\"custom_id\"].str.extract(r\"row-(\\d+)\", expand=False).astype(int)\n",
    "    )\n",
    "\n",
    "    # Flatten the parsed JSON\n",
    "    parsed_flat = pd.json_normalize(ok_rows[\"parsed\"])\n",
    "    scores_flat  = pd.json_normalize(ok_rows[\"parsed\"].apply(lambda d: d.get(\"scores\", {}))).add_prefix(\"score__\")\n",
    "\n",
    "    out = pd.concat(\n",
    "        [ok_rows[[\"row_index\"]], parsed_flat, scores_flat],\n",
    "        axis=1\n",
    "    ).rename(columns={\n",
    "        \"primary_category\": \"primary_category\",\n",
    "        \"secondary_categories\": \"secondary_categories\",\n",
    "        \"newsletter_score\": \"newsletter_score\",\n",
    "        \"scores\": \"scores_raw\"\n",
    "    })\n",
    "\n",
    "    # Errors (non-200 or explicit error field)\n",
    "    err_mask = ~raw[\"response\"].apply(ok) | raw.get(\"error\", pd.Series([None]*len(raw))).notna()\n",
    "    errors = raw[err_mask][[\"custom_id\", \"response\"]].copy()\n",
    "    if \"error\" in raw.columns:\n",
    "        errors[\"error\"] = raw[\"error\"]\n",
    "\n",
    "    return out, errors\n",
    "\n",
    "# Use it\n",
    "parsed_df, errors_df = parse_batch_results(OUTPUT_JSONL)\n",
    "\n",
    "# Merge back on your original df (built earlier in the script)\n",
    "df = df.merge(parsed_df, left_on=\"_row_index\", right_on=\"row_index\", how=\"left\").drop(columns=[\"row_index\"])\n",
    "\n",
    "# Optional: keep just the columns you care about\n",
    "# df = df[\n",
    "#   list(df.columns[:df.columns.get_loc(\"_row_index\")]) +\n",
    "#   [\"primary_category\", \"secondary_categories\", \"newsletter_score\"] +\n",
    "#   [c for c in df.columns if c.startswith(\"score__\")]\n",
    "# ]\n",
    "\n",
    "# Save\n",
    "# df.drop(columns=[\"_row_index\"]).to_parquet(\"news_taxonomy_labeled.parquet\", index=False)\n",
    "df.drop(columns=[\"_row_index\"]).to_csv(\"news_taxonomy_labeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac590ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "_row_index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "created_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "source_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "application_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tools_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "extra_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "techniques_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "short_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "full_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year_week",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "primary_category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "secondary_categories",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "newsletter_score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "scores.research_highlights",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "scores.industry_news",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "scores.cool_use_cases",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "scores.engineering_deep_dives",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "scores.product_launches_tools",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score__research_highlights",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score__industry_news",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score__cool_use_cases",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score__engineering_deep_dives",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score__product_launches_tools",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ef49e04a-fa08-45f2-8b89-963969ccd00b",
       "rows": [
        [
         "0",
         "362",
         "2025-05-07 07:52:00+00:00",
         "Using GenAI to Automatically Fix Java Resource Leaks",
         "Tech",
         "2024.0",
         "https://www.uber.com/en-NL/blog/fixrleak-fixing-java-resource-leaks-with-genai/",
         "uber",
         "code_generation,code_interpretation",
         "cicd,documentation,guardrails",
         "genai,static analysis,ast,java,code generation,prompt engineering,testing,sonarqube,gpt-4,automation,code review",
         "prompt_engineering,error_handling",
         "Uber developed FixrLeak, a framework combining generative AI and Abstract Syntax Tree (AST) analysis to automatically detect and fix resource leaks in Java code. The system processes resource leaks identified by SonarQube, analyzes code safety through AST, and uses GPT-4 to generate appropriate fixes. When tested on 124 resource leaks in Uber's codebase, FixrLeak successfully automated fixes for 93 out of 102 eligible cases, significantly reducing manual intervention while maintaining code quality.",
         "# Uber: Using GenAI to Automatically Fix Java Resource Leaks (2024)\n\nhttps://www.uber.com/en-NL/blog/fixrleak-fixing-java-resource-leaks-with-genai/\n\n## Short Summary\n\nUber developed FixrLeak, a framework combining generative AI and Abstract Syntax Tree (AST) analysis to automatically detect and fix resource leaks in Java code. The system processes resource leaks identified by SonarQube, analyzes code safety through AST, and uses GPT-4 to generate appropriate fixes. When tested on 124 resource leaks in Uber's codebase, FixrLeak successfully automated fixes for 93 out of 102 eligible cases, significantly reducing manual intervention while maintaining code quality.\n\n## Long Summary\n\nThis case study from Uber demonstrates a practical application of generative AI in production software engineering, specifically addressing the persistent challenge of resource leaks in Java applications. The case study is particularly noteworthy as it shows how GenAI can be integrated into existing development workflows to solve real-world problems while maintaining high quality standards and safety guarantees.\n\nAt its core, FixrLeak represents a sophisticated LLMOps implementation that combines traditional software engineering practices with modern AI capabilities. The system demonstrates several key aspects of successful LLM deployment in production:\n\nSystem Architecture and Integration\nFixrLeak's architecture showcases a well-thought-out approach to integrating GenAI into existing development tools and workflows. The system starts with SonarQube for initial leak detection, then uses Tree-sitter for code parsing and AST analysis, before finally leveraging GPT-4 for fix generation. This multi-stage pipeline ensures that the AI model operates within well-defined constraints and only attempts fixes on appropriate cases.\n\nSafety and Validation\nOne of the most impressive aspects of FixrLeak's implementation is its focus on safety. The system employs several layers of validation:\n\n• AST-level analysis to ensure fixes are only attempted on safe cases where resources don't escape their function scope\n• Pre-submission validation including successful builds and test runs\n• SonarQube re-verification to confirm leak resolution\n• Human review as a final safety check\nThis multi-layered approach to validation is crucial for production AI systems, especially when dealing with code modifications.\n\nPrompt Engineering and Model Usage\nThe case study demonstrates sophisticated prompt engineering practices, though specific details of the prompts are not provided. The system crafts targeted prompts based on the analyzed code context and desired fix patterns, particularly focusing on modern Java practices like try-with-resources. This shows how domain knowledge can be effectively encoded into prompts to guide LLM outputs.\n\nProduction Results and Metrics\nThe results demonstrate impressive real-world performance:\n\n• Out of 124 initial cases, 112 were in active code\n• 102 cases were eligible after AST analysis\n• 93 cases were successfully fixed\n• This represents a >91% success rate on eligible cases\nThese metrics show both the system's effectiveness and the importance of proper case filtering and validation in production AI systems.\n\nIntegration with Developer Workflow\nFixrLeak is fully integrated into Uber's development workflow:\n\n• Automated pull request generation\n• Integration with existing build and test systems\n• One-click review process for developers\n• Periodic running to catch new resource leaks\nThis integration demonstrates how AI systems can be made part of regular development processes rather than standing as separate tools.\n\nLimitations and Future Improvements\nThe case study is transparent about current limitations, including:\n\n• Only handling intra-procedural cases (resources within single functions)\n• Reliance on SonarQube for initial detection\n• Limited to Java codebase currently\nFuture plans include expanding to inter-procedural analysis, adding GenAI-based leak detection, and supporting additional languages like Golang. This roadmap shows a mature approach to expanding AI capabilities incrementally.\n\nTechnical Implementation Details\nThe implementation combines several technical components:\n\n• Tree-sitter for code parsing and AST generation\n• Deterministic hashing for leak tracking\n• GPT-4 for code generation\n• Integration with build and test systems\n• Pull request automation\nThis technical stack shows how modern AI can be effectively combined with traditional software engineering tools and practices.\n\nBest Practices and Lessons Learned\nThe case study offers valuable insights for other organizations looking to implement similar systems:\n\n• Focus on well-scoped, high-confidence fixes first\n• Implement thorough validation at multiple stages\n• Maintain human oversight while automating routine tasks\n• Use structured code analysis to ensure safety\n• Integrate with existing development workflows\nThe system's success at Uber demonstrates that GenAI can be effectively deployed in production environments when properly constrained and validated. The focus on solving a specific, well-defined problem (resource leaks) rather than attempting more general code fixes likely contributed to its high success rate.\n\nProduction Considerations\nThe case study highlights several important production considerations:\n\n• The need for thorough testing and validation\n• Integration with existing tools and workflows\n• Handling of edge cases and failures\n• Scaling across large codebases\n• Maintaining developer trust through transparency\nOverall, this case study provides a comprehensive example of how to successfully deploy GenAI in a production software engineering environment, balancing automation with safety and quality considerations. The systematic approach to validation and integration, combined with clear metrics and limitations, makes this a valuable reference for other organizations looking to implement similar systems.\n\n\n",
         "2025-05-05/2025-05-11",
         "cool_use_cases",
         "['engineering_deep_dives']",
         "5",
         "1",
         "1",
         "5",
         "4",
         "1",
         "1",
         "1",
         "5",
         "4",
         "1"
        ],
        [
         "1",
         "73",
         "2024-11-18 08:56:00+00:00",
         "Scaling and Optimizing Self-Hosted LLMs for Developer Documentation",
         "Tech",
         "2023.0",
         "https://www.youtube.com/watch?v=886hZl5Qp7g",
         "various",
         "question_answering,document_processing",
         "fastapi,vllm,mistral,scaling,monitoring,documentation,guardrails,security,scalability,reliability",
         "rag,inference optimization,vllm,ray serve,gpu optimization,benchmarking,performance testing,self hosted,mistral,fastapi,guardrails,vector database,horizontal scaling,deployment",
         "rag,model_optimization,latency_optimization,cost_optimization",
         "A tech company needed to improve their developer documentation accessibility and understanding. They implemented a self-hosted LLM solution using retrieval augmented generation (RAG), with guard rails for content safety. The team optimized performance using vLLM for faster inference and Ray Serve for horizontal scaling, achieving significant improvements in latency and throughput while maintaining cost efficiency. The solution helped developers better understand and adopt the company's products while keeping proprietary information secure.",
         "# Various: Scaling and Optimizing Self-Hosted LLMs for Developer Documentation (2023)\n\nhttps://www.youtube.com/watch?v=886hZl5Qp7g\n\n## Short Summary\n\nA tech company needed to improve their developer documentation accessibility and understanding. They implemented a self-hosted LLM solution using retrieval augmented generation (RAG), with guard rails for content safety. The team optimized performance using vLLM for faster inference and Ray Serve for horizontal scaling, achieving significant improvements in latency and throughput while maintaining cost efficiency. The solution helped developers better understand and adopt the company's products while keeping proprietary information secure.\n\n## Long Summary\n\n# Scaling Self-Hosted LLMs for Developer Documentation Search\n\n## Project Overview\n\nA technology company that builds hardware and software platforms for developers needed to improve their developer documentation accessibility. Their existing documentation was difficult to navigate, causing challenges for developers trying to understand their products. The company wanted a self-hosted LLM solution to maintain control over proprietary information and build internal capabilities.\n\n## Technical Architecture\n\n### Core Components\n\n• Orchestration service using FastAPI\n• Vector database for RAG implementation\n• Guard rails service for content safety\n• Model server running Mistral 7B Instruct\n• AWS GPU instances for inference\n### System Flow\n\n• Developer queries enter through orchestration service\n• Vector database retrieves relevant documentation context\n• Guard rails service performs safety checks and topic validation\n• LLM generates responses using retrieved context\n• Custom model trained for topic validation\n## Performance Optimization Approach\n\n### Benchmarking Strategy\n\n• Developed test scenarios using Locust\n• Created progressive load testing scenarios:\n### Key Metrics\n\n• Latency (response time)\n• Throughput (tokens per second)\n• Request rate (successful requests per unit time)\n• Additional tracking:\n## Optimization Solutions\n\n### vLLM Implementation\n\n• Addressed GPU memory bottleneck\n• Implemented PageAttention for key-value lookups\n• Achieved improvements:\n### Horizontal Scaling with Ray Serve\n\n• Enabled multi-server deployment\n• Provided autoscaling capabilities\n• Integrated with cloud provider autoscaling\n• Implemented GPU sharing across services:\n### Integration Benefits\n\n• Successful combination of vLLM and Ray Serve\n• Balanced vertical and horizontal scaling\n• Optimized GPU utilization across services\n## Guard Rails Implementation\n\n### Safety Features\n\n• Topic validation using custom trained model\n• Prevention of:\n• Proprietary information protection\n## Key Learnings and Best Practices\n\n### Performance Optimization\n\n• Always benchmark before optimization\n• Focus on user experience metrics\n• Consider both vertical and horizontal scaling\n• Monitor GPU utilization and costs\n### System Design\n\n• Implement robust guard rails\n• Use RAG for accurate responses\n• Balance performance with cost efficiency\n• Consider GPU sharing across services\n### Development Process\n\n• Start with proof of concept\n• Establish baseline metrics\n• Use progressive load testing\n• Document all configuration changes\n## Future Considerations\n\n### Scalability\n\n• Monitor emerging optimization techniques\n• Evaluate new model deployment strategies\n• Consider alternative GPU sharing approaches\n### Technology Evolution\n\n• Stay current with framework updates\n• Evaluate new model options\n• Monitor cloud provider capabilities\n## Infrastructure Decisions\n\n### Cloud Setup\n\n• AWS GPU instances for inference\n• Consideration of cost vs performance\n• Autoscaling configuration\n• GPU resource sharing\n### Monitoring and Maintenance\n\n• Performance metric tracking\n• Version control integration\n• Environment configuration management\n• Benchmark result documentation\nThis case study demonstrates the importance of systematic performance optimization in LLM deployments, combining both vertical optimization through vLLM and horizontal scaling through Ray Serve. The implementation of comprehensive guard rails and careful attention to benchmarking resulted in a production-ready system that effectively serves developer documentation needs while maintaining security and performance standards.\n\n\n",
         "2024-11-18/2024-11-24",
         "cool_use_cases",
         "['engineering_deep_dives']",
         "4",
         "1",
         "1",
         "5",
         "4",
         "1",
         "1",
         "1",
         "5",
         "4",
         "1"
        ]
       ],
       "shape": {
        "columns": 27,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_row_index</th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>industry</th>\n",
       "      <th>year</th>\n",
       "      <th>source_url</th>\n",
       "      <th>company</th>\n",
       "      <th>application_tags</th>\n",
       "      <th>tools_tags</th>\n",
       "      <th>extra_tags</th>\n",
       "      <th>...</th>\n",
       "      <th>scores.research_highlights</th>\n",
       "      <th>scores.industry_news</th>\n",
       "      <th>scores.cool_use_cases</th>\n",
       "      <th>scores.engineering_deep_dives</th>\n",
       "      <th>scores.product_launches_tools</th>\n",
       "      <th>score__research_highlights</th>\n",
       "      <th>score__industry_news</th>\n",
       "      <th>score__cool_use_cases</th>\n",
       "      <th>score__engineering_deep_dives</th>\n",
       "      <th>score__product_launches_tools</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>362</td>\n",
       "      <td>2025-05-07 07:52:00+00:00</td>\n",
       "      <td>Using GenAI to Automatically Fix Java Resource...</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>https://www.uber.com/en-NL/blog/fixrleak-fixin...</td>\n",
       "      <td>uber</td>\n",
       "      <td>code_generation,code_interpretation</td>\n",
       "      <td>cicd,documentation,guardrails</td>\n",
       "      <td>genai,static analysis,ast,java,code generation...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>2024-11-18 08:56:00+00:00</td>\n",
       "      <td>Scaling and Optimizing Self-Hosted LLMs for De...</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=886hZl5Qp7g</td>\n",
       "      <td>various</td>\n",
       "      <td>question_answering,document_processing</td>\n",
       "      <td>fastapi,vllm,mistral,scaling,monitoring,docume...</td>\n",
       "      <td>rag,inference optimization,vllm,ray serve,gpu ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _row_index                 created_at  \\\n",
       "0         362  2025-05-07 07:52:00+00:00   \n",
       "1          73  2024-11-18 08:56:00+00:00   \n",
       "\n",
       "                                               title industry    year  \\\n",
       "0  Using GenAI to Automatically Fix Java Resource...     Tech  2024.0   \n",
       "1  Scaling and Optimizing Self-Hosted LLMs for De...     Tech  2023.0   \n",
       "\n",
       "                                          source_url  company  \\\n",
       "0  https://www.uber.com/en-NL/blog/fixrleak-fixin...     uber   \n",
       "1        https://www.youtube.com/watch?v=886hZl5Qp7g  various   \n",
       "\n",
       "                         application_tags  \\\n",
       "0     code_generation,code_interpretation   \n",
       "1  question_answering,document_processing   \n",
       "\n",
       "                                          tools_tags  \\\n",
       "0                      cicd,documentation,guardrails   \n",
       "1  fastapi,vllm,mistral,scaling,monitoring,docume...   \n",
       "\n",
       "                                          extra_tags  ...  \\\n",
       "0  genai,static analysis,ast,java,code generation...  ...   \n",
       "1  rag,inference optimization,vllm,ray serve,gpu ...  ...   \n",
       "\n",
       "  scores.research_highlights scores.industry_news scores.cool_use_cases  \\\n",
       "0                          1                    1                     5   \n",
       "1                          1                    1                     5   \n",
       "\n",
       "  scores.engineering_deep_dives scores.product_launches_tools  \\\n",
       "0                             4                             1   \n",
       "1                             4                             1   \n",
       "\n",
       "  score__research_highlights  score__industry_news  score__cool_use_cases  \\\n",
       "0                          1                     1                      5   \n",
       "1                          1                     1                      5   \n",
       "\n",
       "   score__engineering_deep_dives  score__product_launches_tools  \n",
       "0                              4                              1  \n",
       "1                              4                              1  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b13474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_row_index\":362,\n",
      "  \"created_at\":\"2025-05-07 07:52:00+00:00\",\n",
      "  \"title\":\"Using GenAI to Automatically Fix Java Resource Leaks\",\n",
      "  \"industry\":\"Tech\",\n",
      "  \"year\":2024.0,\n",
      "  \"source_url\":\"https:\\/\\/www.uber.com\\/en-NL\\/blog\\/fixrleak-fixing-java-resource-leaks-with-genai\\/\",\n",
      "  \"company\":\"uber\",\n",
      "  \"application_tags\":\"code_generation,code_interpretation\",\n",
      "  \"tools_tags\":\"cicd,documentation,guardrails\",\n",
      "  \"extra_tags\":\"genai,static analysis,ast,java,code generation,prompt engineering,testing,sonarqube,gpt-4,automation,code review\",\n",
      "  \"techniques_tags\":\"prompt_engineering,error_handling\",\n",
      "  \"short_summary\":\"Uber developed FixrLeak, a framework combining generative AI and Abstract Syntax Tree (AST) analysis to automatically detect and fix resource leaks in Java code. The system processes resource leaks identified by SonarQube, analyzes code safety through AST, and uses GPT-4 to generate appropriate fixes. When tested on 124 resource leaks in Uber's codebase, FixrLeak successfully automated fixes for 93 out of 102 eligible cases, significantly reducing manual intervention while maintaining code quality.\",\n",
      "  \"full_summary\":\"# Uber: Using GenAI to Automatically Fix Java Resource Leaks (2024)\\n\\nhttps:\\/\\/www.uber.com\\/en-NL\\/blog\\/fixrleak-fixing-java-resource-leaks-with-genai\\/\\n\\n## Short Summary\\n\\nUber developed FixrLeak, a framework combining generative AI and Abstract Syntax Tree (AST) analysis to automatically detect and fix resource leaks in Java code. The system processes resource leaks identified by SonarQube, analyzes code safety through AST, and uses GPT-4 to generate appropriate fixes. When tested on 124 resource leaks in Uber's codebase, FixrLeak successfully automated fixes for 93 out of 102 eligible cases, significantly reducing manual intervention while maintaining code quality.\\n\\n## Long Summary\\n\\nThis case study from Uber demonstrates a practical application of generative AI in production software engineering, specifically addressing the persistent challenge of resource leaks in Java applications. The case study is particularly noteworthy as it shows how GenAI can be integrated into existing development workflows to solve real-world problems while maintaining high quality standards and safety guarantees.\\n\\nAt its core, FixrLeak represents a sophisticated LLMOps implementation that combines traditional software engineering practices with modern AI capabilities. The system demonstrates several key aspects of successful LLM deployment in production:\\n\\nSystem Architecture and Integration\\nFixrLeak's architecture showcases a well-thought-out approach to integrating GenAI into existing development tools and workflows. The system starts with SonarQube for initial leak detection, then uses Tree-sitter for code parsing and AST analysis, before finally leveraging GPT-4 for fix generation. This multi-stage pipeline ensures that the AI model operates within well-defined constraints and only attempts fixes on appropriate cases.\\n\\nSafety and Validation\\nOne of the most impressive aspects of FixrLeak's implementation is its focus on safety. The system employs several layers of validation:\\n\\n\\u2022 AST-level analysis to ensure fixes are only attempted on safe cases where resources don't escape their function scope\\n\\u2022 Pre-submission validation including successful builds and test runs\\n\\u2022 SonarQube re-verification to confirm leak resolution\\n\\u2022 Human review as a final safety check\\nThis multi-layered approach to validation is crucial for production AI systems, especially when dealing with code modifications.\\n\\nPrompt Engineering and Model Usage\\nThe case study demonstrates sophisticated prompt engineering practices, though specific details of the prompts are not provided. The system crafts targeted prompts based on the analyzed code context and desired fix patterns, particularly focusing on modern Java practices like try-with-resources. This shows how domain knowledge can be effectively encoded into prompts to guide LLM outputs.\\n\\nProduction Results and Metrics\\nThe results demonstrate impressive real-world performance:\\n\\n\\u2022 Out of 124 initial cases, 112 were in active code\\n\\u2022 102 cases were eligible after AST analysis\\n\\u2022 93 cases were successfully fixed\\n\\u2022 This represents a >91% success rate on eligible cases\\nThese metrics show both the system's effectiveness and the importance of proper case filtering and validation in production AI systems.\\n\\nIntegration with Developer Workflow\\nFixrLeak is fully integrated into Uber's development workflow:\\n\\n\\u2022 Automated pull request generation\\n\\u2022 Integration with existing build and test systems\\n\\u2022 One-click review process for developers\\n\\u2022 Periodic running to catch new resource leaks\\nThis integration demonstrates how AI systems can be made part of regular development processes rather than standing as separate tools.\\n\\nLimitations and Future Improvements\\nThe case study is transparent about current limitations, including:\\n\\n\\u2022 Only handling intra-procedural cases (resources within single functions)\\n\\u2022 Reliance on SonarQube for initial detection\\n\\u2022 Limited to Java codebase currently\\nFuture plans include expanding to inter-procedural analysis, adding GenAI-based leak detection, and supporting additional languages like Golang. This roadmap shows a mature approach to expanding AI capabilities incrementally.\\n\\nTechnical Implementation Details\\nThe implementation combines several technical components:\\n\\n\\u2022 Tree-sitter for code parsing and AST generation\\n\\u2022 Deterministic hashing for leak tracking\\n\\u2022 GPT-4 for code generation\\n\\u2022 Integration with build and test systems\\n\\u2022 Pull request automation\\nThis technical stack shows how modern AI can be effectively combined with traditional software engineering tools and practices.\\n\\nBest Practices and Lessons Learned\\nThe case study offers valuable insights for other organizations looking to implement similar systems:\\n\\n\\u2022 Focus on well-scoped, high-confidence fixes first\\n\\u2022 Implement thorough validation at multiple stages\\n\\u2022 Maintain human oversight while automating routine tasks\\n\\u2022 Use structured code analysis to ensure safety\\n\\u2022 Integrate with existing development workflows\\nThe system's success at Uber demonstrates that GenAI can be effectively deployed in production environments when properly constrained and validated. The focus on solving a specific, well-defined problem (resource leaks) rather than attempting more general code fixes likely contributed to its high success rate.\\n\\nProduction Considerations\\nThe case study highlights several important production considerations:\\n\\n\\u2022 The need for thorough testing and validation\\n\\u2022 Integration with existing tools and workflows\\n\\u2022 Handling of edge cases and failures\\n\\u2022 Scaling across large codebases\\n\\u2022 Maintaining developer trust through transparency\\nOverall, this case study provides a comprehensive example of how to successfully deploy GenAI in a production software engineering environment, balancing automation with safety and quality considerations. The systematic approach to validation and integration, combined with clear metrics and limitations, makes this a valuable reference for other organizations looking to implement similar systems.\\n\\n\\n\",\n",
      "  \"year_week\":\"2025-05-05\\/2025-05-11\",\n",
      "  \"primary_category\":\"cool_use_cases\",\n",
      "  \"secondary_categories\":[\n",
      "    \"engineering_deep_dives\"\n",
      "  ],\n",
      "  \"newsletter_score\":5,\n",
      "  \"scores.research_highlights\":1,\n",
      "  \"scores.industry_news\":1,\n",
      "  \"scores.cool_use_cases\":5,\n",
      "  \"scores.engineering_deep_dives\":4,\n",
      "  \"scores.product_launches_tools\":1,\n",
      "  \"score__research_highlights\":1,\n",
      "  \"score__industry_news\":1,\n",
      "  \"score__cool_use_cases\":5,\n",
      "  \"score__engineering_deep_dives\":4,\n",
      "  \"score__product_launches_tools\":1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print the first row in a json format\n",
    "print(df.iloc[0].to_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
