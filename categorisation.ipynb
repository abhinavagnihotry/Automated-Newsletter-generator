{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24fba573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "created_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "source_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "application_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tools_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "extra_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "techniques_tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "short_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "full_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year_month",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "240d8d11-ffb9-4a7f-9f30-873c43b29007",
       "rows": [
        [
         "0",
         "2024-07-10 14:21:00+00:00",
         "LLM Validation and Testing at Scale: GitLab's Comprehensive Model Evaluation Framework",
         "Tech",
         "2024.0",
         "https://about.gitlab.com/blog/2024/05/09/developing-gitlab-duo-how-we-validate-and-test-ai-models-at-scale/",
         "gitlab",
         "code_generation,high_stakes_application,regulatory_compliance",
         "monitoring,cicd,devops,continuous_deployment,continuous_integration,documentation,security,compliance,guardrails,reliability,scalability",
         "llm,testing,evaluation,prompt engineering,metrics,validation,deployment,model selection,quality assurance,continuous validation",
         "prompt_engineering,error_handling,latency_optimization,system_prompts",
         "GitLab developed a robust framework for validating and testing LLMs at scale for their GitLab Duo AI features. They created a Centralized Evaluation Framework (CEF) that uses thousands of prompts across multiple use cases to assess model performance. The process involves creating a comprehensive prompt library, establishing baseline model performance, iterative feature development, and continuous validation using metrics like Cosine Similarity Score and LLM Judge, ensuring consistent improvement while maintaining quality across all use cases.",
         "# Gitlab: LLM Validation and Testing at Scale: GitLab's Comprehensive Model Evaluation Framework (2024)\n\nhttps://about.gitlab.com/blog/2024/05/09/developing-gitlab-duo-how-we-validate-and-test-ai-models-at-scale/\n\n## Short Summary\n\nGitLab developed a robust framework for validating and testing LLMs at scale for their GitLab Duo AI features. They created a Centralized Evaluation Framework (CEF) that uses thousands of prompts across multiple use cases to assess model performance. The process involves creating a comprehensive prompt library, establishing baseline model performance, iterative feature development, and continuous validation using metrics like Cosine Similarity Score and LLM Judge, ensuring consistent improvement while maintaining quality across all use cases.\n\n## Long Summary\n\n# GitLab's Approach to LLM Testing and Validation\n\n## Company Overview\n\nGitLab, a leading DevSecOps platform provider, has implemented AI features called GitLab Duo across their platform. They use foundation models from Google and Anthropic, maintaining flexibility by not being tied to a single provider. This case study details their sophisticated approach to validating and testing AI models at scale.\n\n## Technical Infrastructure\n\n### Centralized Evaluation Framework (CEF)\n\n• Core testing infrastructure that processes thousands of prompts\n• Covers dozens of use cases\n• Designed to identify significant patterns in LLM behavior\n• Enables comprehensive assessment of foundational LLMs and integrated features\n### Model Testing and Validation Process\n\n### Prompt Library Development\n\n• Created as a proxy for production data\n• Does not use customer data for training\n• Consists of carefully crafted question-answer pairs\n• Questions represent expected production queries\n• Answers serve as ground truth for evaluation\n• Specifically designed for GitLab features and use cases\n### Performance Metrics\n\n• Implements multiple evaluation metrics:\n• Continuously updates evaluation techniques based on industry developments\n### Testing Methodology\n\n• Systematic approach to scale testing:\n• Daily validation during active development\n• Iterative improvement process\n### Feature Development Workflow\n\n### Baseline Establishment\n\n• Initial performance measurement of various models\n• Comparison against ground truth answers\n• Selection of appropriate foundation models based on performance metrics\n### Iterative Development Process\n\n• Pattern identification in test results\n• Analysis of:\n### Optimization Strategy\n\n• Creation of focused subset datasets for rapid iteration\n• Weighted testing data including:\n• Validation against multiple data subsets\n• Continuous performance monitoring against baseline\n## Quality Assurance Measures\n\n### Testing Priorities\n\n• Ensuring consistent quality across features\n• Optimizing model performance\n• Maintaining reliability in production\n• Addressing potential biases and anomalies\n• Security vulnerability assessment\n• Ethical consideration validation\n### Validation Process\n\n• Daily performance validation\n• Comprehensive metrics tracking\n• Impact assessment of changes\n• Cross-feature performance monitoring\n## Implementation Details\n\n### Feature Integration\n\n• Currently powers multiple AI features:\n• Integrated validation process in development pipeline\n• Continuous improvement methodology\n### Risk Management\n\n• Thorough testing across diverse datasets\n• Identification of potential failure modes\n• Security vulnerability assessment\n• Ethical consideration validation\n• Performance impact monitoring\n## Best Practices and Lessons Learned\n\n### Key Insights\n\n• Importance of scale in testing\n• Need for representative test data\n• Value of iterative validation\n• Balance between targeted and broad testing\n• Significance of continuous monitoring\n### Challenges Addressed\n\n• Handling subjective and variable interpretations\n• Managing stochastic nature of outputs\n• Balancing improvement with stability\n• Avoiding overfitting in prompt engineering\n• Maintaining performance across features\n## Results and Impact\n\n### Achievements\n\n• Successfully deployed multiple AI features\n• Established robust validation framework\n• Implemented continuous improvement process\n• Maintained high quality standards\n• Created scalable testing infrastructure\n### Ongoing Development\n\n• Regular feature iterations\n• Continuous performance monitoring\n• Adaptation to new use cases\n• Integration of new evaluation techniques\n• Response to emerging challenges\n## Future Considerations\n\n### Development Plans\n\n• Expansion of testing framework\n• Integration of new metrics\n• Enhancement of validation processes\n• Adaptation to emerging AI technologies\n• Scaling of testing infrastructure\n### Strategic Focus\n\n• Maintaining testing efficiency\n• Ensuring comprehensive coverage\n• Adapting to new use cases\n• Incorporating industry best practices\n\n",
         "2024-07-08/2024-07-14"
        ],
        [
         "1",
         "2024-07-10 14:21:00+00:00",
         "Building a Scalable Retriever-Ranker Architecture: Malt's Journey with Vector Databases and LLM-Powered Freelancer Matching",
         "Tech",
         "2024.0",
         "https://blog.malt.engineering/super-powering-our-freelancer-recommendation-system-using-a-vector-database-add643fcfd23",
         "malt",
         "structured_output,realtime_application",
         "kubernetes,monitoring,scaling,devops,orchestration,reliability,scalability,elasticsearch,qdrant",
         "recommenders,rerankers,vector database,qdrant,kubernetes,prometheus,grafana,sharding,embeddings",
         "embeddings,semantic_search,vector_search,model_optimization,latency_optimization",
         "Malt's implementation of a retriever-ranker architecture for their freelancer recommendation system, leveraging a vector database (Qdrant) to improve matching speed and scalability. The case study highlights the importance of carefully selecting and integrating vector databases in LLM-powered systems, emphasizing performance benchmarking, filtering capabilities, and deployment considerations to achieve significant improvements in response times and recommendation quality.",
         "# Malt: Building a Scalable Retriever-Ranker Architecture: Malt's Journey with Vector Databases and LLM-Powered Freelancer Matching (2024)\n\nhttps://blog.malt.engineering/super-powering-our-freelancer-recommendation-system-using-a-vector-database-add643fcfd23\n\n## Short Summary\n\nMalt's implementation of a retriever-ranker architecture for their freelancer recommendation system, leveraging a vector database (Qdrant) to improve matching speed and scalability. The case study highlights the importance of carefully selecting and integrating vector databases in LLM-powered systems, emphasizing performance benchmarking, filtering capabilities, and deployment considerations to achieve significant improvements in response times and recommendation quality.\n\n## Long Summary\n\n# Malt's Vector Database-Powered Recommendation System: Technical Summary\n\n## Background & Challenge\n\n• Platform Purpose: Malt connects freelancers with projects\n• Initial System (2021):\n## New Architecture: The Retriever-Ranker Solution\n\n### 1. Retriever Step\n\n• Implements bi-encoder model for initial filtering\n• Features:\n### 2. Ranker Step\n\n• Employs cross-encoder model for precise matching\n• Process:\n## Vector Database Implementation\n\n### Technology Selection\n\nBenchmarked multiple options:\n\nSelected Qdrant for:\n\n• Optimal balance of speed and accuracy\n• Strong filtering capabilities\n• Excellent scalability\n## Technical Implementation Details\n\n• Model Architecture: Transformer-based encoding\n• Deployment:\n• Monitoring: Prometheus/Grafana integration\n## Results & Improvements\n\n• Response Time: Reduced from 60+ seconds to under 3 seconds\n• Quality: Maintained or improved recommendation accuracy\n• Scalability: Successfully handles 700,000+ freelancer profiles\n• Functionality: Enables real-time recommendations\n## Future Development Plans\n\n## Key Takeaways\n\nThis new system represents a significant advancement in Malt's matching capabilities, providing both immediate benefits and a foundation for future enhancements.\n\n## Model Architecture Decisions\n1. Two-Tier Architecture\n- Split computation between fast retrieval and precise ranking\n- Enables better scaling and maintenance\n- Allows independent optimization of each component\n\n2. Custom Embedding Training\n- Built specialized models rather than using generic embeddings\n- Confirmed value of domain-specific training\n- Better performance than off-the-shelf models like Sentence-Transformers\n\n## Infrastructure & Deployment\n1. Vector Database Selection\n- Systematic evaluation of options (Elasticsearch, PGVector, Qdrant)\n- Key criteria:\n- Query performance\n- ANN algorithm quality\n- Filtering capabilities\n- Scalability requirements\n\n2. Production Architecture\n- Kubernetes-based deployment\n- Cluster configuration with sharding for scalability\n- Replication for high availability\n- Prometheus/Grafana monitoring integration\n\n## Performance Optimization\n1. Vector Search Optimization\n- Implemented Approximate Nearest Neighbor (ANN) search\n- Pre-computation of embeddings for freelancer profiles\n- Real-time computation only for new projects\n- Balanced recall vs precision tradeoffs\n\n2. System Monitoring\n- Real-time performance metrics tracking\n- Latency monitoring (p95 measurements)\n- System health monitoring\n- Resource utilization tracking\n\n## Testing & Validation\n1. Benchmark Testing\n- Systematic comparison of different solutions\n- Used standardized dataset (GIST1M Texmex corpus)\n- Measured multiple metrics:\n- Queries per second\n- ANN precision\n- Filtering capability\n\n2. Production Validation\n- A/B testing for quality verification\n- Conversion tracking\n- Response time monitoring\n- Quality metrics maintenance\n\n## Key LLMOps Learnings\n1. Architecture Design\n- Modular design enables easier updates and maintenance\n- Separate fast path (retriever) from precise path (ranker)\n- Consider both current and future scaling needs\n\n2. Model Management\n- Value of custom-trained models for specific use cases\n- Importance of maintaining vector database freshness\n- Need for regular model updates and version management\n\n3. Production Considerations\n- Importance of monitoring and observability\n- Need for robust filtering capabilities\n- Value of gradual deployment (retriever first, then ranker)\n\n## Future Considerations\n1. Hybrid Search Development\n- Combining semantic and keyword searches\n- Integration with traditional search capabilities\n- Balance between different search methodologies\n\n2. Scaling Strategies\n- Design for horizontal scaling\n- Consider data freshness requirements\n- Plan for increasing data volumes\n\n## Best Practices Identified\n1. Development\n- Start with simpler architecture and iterate\n- Test thoroughly with production-scale data\n- Build monitoring from the start\n\n2. Deployment\n- Use containerization for consistency\n- Implement robust monitoring\n- Plan for high availability\n- Consider geographical distribution\n\n3. Maintenance\n- Regular performance monitoring\n- Systematic update procedures\n- Continuous model improvement\n- Regular benchmarking against alternative\n\n\n",
         "2024-07-08/2024-07-14"
        ],
        [
         "2",
         "2024-07-10 14:38:00+00:00",
         "Building Secure and Private Enterprise LLM Infrastructure",
         "Tech",
         "2024.0",
         "https://slack.engineering/how-we-built-slack-ai-to-be-secure-and-private/",
         "slack",
         "regulatory_compliance,legacy_system_integration",
         "security,compliance,guardrails,reliability,scalability,monitoring,databases,load_balancing,serverless",
         "rag,aws,sagemaker,security,privacy,compliance,llm,vpc,acl,dlp,encryption",
         "rag,semantic_search,error_handling,latency_optimization,cost_optimization,fallback_strategies,system_prompts",
         "Slack implemented AI features by developing a secure architecture that ensures customer data privacy and compliance. They used AWS SageMaker to host LLMs in their VPC, implemented RAG instead of fine-tuning models, and maintained strict data access controls. The solution resulted in 90% of AI-adopting users reporting increased productivity while maintaining enterprise-grade security and compliance requirements.",
         "# Slack: Building Secure and Private Enterprise LLM Infrastructure (2024)\n\nhttps://slack.engineering/how-we-built-slack-ai-to-be-secure-and-private/\n\n## Short Summary\n\nSlack implemented AI features by developing a secure architecture that ensures customer data privacy and compliance. They used AWS SageMaker to host LLMs in their VPC, implemented RAG instead of fine-tuning models, and maintained strict data access controls. The solution resulted in 90% of AI-adopting users reporting increased productivity while maintaining enterprise-grade security and compliance requirements.\n\n## Long Summary\n\n# Building Secure Enterprise LLM Infrastructure at Slack\n\n## Company Overview and Challenge\n\nSlack, a leading enterprise communication platform, faced the challenge of implementing AI features while maintaining their strict security standards and privacy requirements. As a FedRAMP Moderate authorized platform, they needed to ensure that their AI implementation would meet rigorous compliance requirements while delivering value to their users.\n\n## Technical Architecture and Implementation\n\n### Core Principles\n\nThe implementation was guided by four key principles:\n\n• Customer data must never leave Slack\n• No training of LLMs on customer data\n• AI operations limited to user-accessible data only\n• Maintenance of enterprise-grade security and compliance\n### Infrastructure Setup\n\n• Utilized AWS SageMaker to host closed-source LLMs in an escrow VPC\n• Implemented complete control over customer data lifecycle\n• Prevented model providers from accessing customer data\n• Maintained data within Slack-controlled AWS VPCs\n### Model Strategy and RAG Implementation\n\n• Chose to use off-the-shelf models instead of fine-tuning\n• Implemented Retrieval Augmented Generation (RAG) for stateless operation\n• Selected models based on context window size and latency requirements\n• Combined traditional ML models with generative models for improved results\n### Security and Access Control Features\n\n• Built on existing Slack security infrastructure\n• Implemented strict access control using user ACLs\n• AI-generated outputs visible only to the requesting user\n• Integrated with existing compliance features:\n### Data Privacy Measures\n\n• Ephemeral storage for AI outputs where possible\n• Minimal data storage following least-data principle\n• Integration with existing tombstoning mechanisms\n• Automatic invalidation of derived content when source content is removed\n## Technical Challenges and Solutions\n\n### Model Selection Challenges\n\n• Required large context windows for processing channel data\n• Needed to balance latency with processing capability\n• Evaluated multiple models for summarization and search use cases\n### Integration Challenges\n\n• Built on top of existing Slack feature sets\n• Reused existing security libraries and infrastructure\n• Created new compliance support for derived content\n• Implemented special handling for DLP and administrative controls\n### Data Processing\n\n• Implemented context-aware processing for channel summarization\n• Built secure data fetching mechanisms using existing ACLs\n• Developed systems for handling spiky demand\n• Created evaluation frameworks for model performance\n## Implementation Results and Impact\n\n### Security Achievements\n\n• Successfully maintained FedRAMP Moderate authorization\n• Prevented customer data leakage outside trust boundary\n• Implemented comprehensive compliance controls\n• Maintained existing security standards\n### Performance Outcomes\n\n• 90% of AI feature adopters reported higher productivity\n• Successfully addressed key user pain points:\n### Architecture Benefits\n\n• Scalable and secure AI infrastructure\n• Maintenance of data privacy guarantees\n• Enterprise-grade security compliance\n• Flexible foundation for future AI features\n## Technical Best Practices and Learnings\n\n### Security First Approach\n\n• Start with security requirements first\n• Build on existing security infrastructure\n• Implement strict data access controls\n• Maintain compliance requirements throughout\n### Architecture Decisions\n\n• Use of RAG over fine-tuning for privacy\n• Hosting models in controlled environments\n• Implementation of stateless processing\n• Integration with existing security frameworks\n### Data Handling\n\n• Implement strict data lifecycle management\n• Use ephemeral storage where possible\n• Maintain detailed data lineage\n• Integrate with existing data protection mechanisms\n### Future Considerations\n\n• Monitoring and evaluation of model performance\n• Handling increasing context window sizes\n• Managing prompt optimization\n• Scaling for demand spikes\n## Infrastructure and Tooling\n\n### AWS Integration\n\n• SageMaker for model hosting\n• VPC configuration for data security\n• Integration with existing AWS infrastructure\n• Compliance maintenance in cloud environment\n### Security Tools\n\n• Access Control Lists (ACLs)\n• Data Loss Protection (DLP)\n• Encryption key management\n• Data residency controls\n### Development Framework\n\n• Built on existing Slack infrastructure\n• Integration with core services\n• Reuse of security libraries\n• Custom compliance tooling\nThis case study demonstrates how enterprise-grade AI features can be implemented while maintaining strict security and compliance requirements. Slack's approach shows that with careful architecture and implementation choices, organizations can leverage powerful AI capabilities while protecting sensitive customer data.\n\n\n",
         "2024-07-08/2024-07-14"
        ],
        [
         "3",
         "2024-07-10 14:39:00+00:00",
         "Building and Scaling LLM Applications at Discord",
         "Tech",
         "2024.0",
         "https://discord.com/blog/developing-rapidly-with-generative-ai",
         "discord",
         "chatbot,content_moderation,structured_output,realtime_application,regulatory_compliance",
         "monitoring,scaling,devops,security,compliance,guardrails,reliability,scalability",
         "prompt engineering,evaluation,deployment,safety,triton,vllm,llama,mistral,fine tuning,inference,gpt-4,chatgpt",
         "prompt_engineering,fine_tuning,model_optimization,error_handling,latency_optimization,cost_optimization,fallback_strategies",
         "Discord shares their comprehensive approach to building and deploying LLM-powered features, from ideation to production. They detail their process of identifying use cases, defining requirements, prototyping with commercial LLMs, evaluating prompts using AI-assisted evaluation, and ultimately scaling through either hosted or self-hosted solutions. The case study emphasizes practical considerations around latency, quality, safety, and cost optimization while building production LLM applications.",
         "# Discord: Building and Scaling LLM Applications at Discord (2024)\n\nhttps://discord.com/blog/developing-rapidly-with-generative-ai\n\n## Short Summary\n\nDiscord shares their comprehensive approach to building and deploying LLM-powered features, from ideation to production. They detail their process of identifying use cases, defining requirements, prototyping with commercial LLMs, evaluating prompts using AI-assisted evaluation, and ultimately scaling through either hosted or self-hosted solutions. The case study emphasizes practical considerations around latency, quality, safety, and cost optimization while building production LLM applications.\n\n## Long Summary\n\n# Discord's LLM Application Development Framework\n\nDiscord, a leading communication platform, presents a comprehensive framework for developing and deploying LLM-powered features at scale. This case study provides valuable insights into their systematic approach to implementing generative AI solutions in production environments.\n\n## Initial Assessment and Planning\n\n### Use Case Identification\n\n• Focus on problems that involve:\n### Requirements Definition\n\n• Key considerations include:\n## Prototyping and Development Process\n\n### Model Selection Strategy\n\n• Initial preference for advanced commercial LLMs\n• Focus on product iteration rather than infrastructure development\n### Prompt Engineering and Evaluation\n\n• Systematic approach to prompt development:\n• AI-assisted evaluation methodology:\n### Testing and Iteration\n\n• Limited release methodology:\n• Key metrics tracked:\n## Production Deployment Architecture\n\n### Core Infrastructure Components\n\n• Input processing and prompt preparation\n• LLM inference server integration\n• Content safety filtering\n• Output processing and validation\n• Monitoring and logging systems\n### Safety and Privacy Considerations\n\n• Implementation of content safety filters\n• Integration with trust and safety ML models\n• Collaboration with Legal and Safety teams\n• Adherence to data minimization principles\n### Self-Hosted LLM Implementation\n\n• Considerations for self-hosting:\n### Infrastructure Optimization\n\n• Model server configuration:\n• Model selection considerations:\n## Technical Challenges and Solutions\n\n### Performance Optimization\n\n• Balance between model capability and latency\n• Throughput optimization through batching\n• GPU utilization optimization\n• Infrastructure scaling considerations\n### Cost Management\n\n• Token usage monitoring\n• Infrastructure cost optimization\n• Balance between hosted and self-hosted solutions\n### Quality Assurance\n\n• Output format consistency\n• Error rate monitoring\n• Hallucination detection and mitigation\n• Structured output parsing\n### Safety and Privacy\n\n• Input sanitization\n• Output content filtering\n• Privacy-preserving processing\n• Regulatory compliance\n## Best Practices and Lessons Learned\n\n### Development Approach\n\n• Start with commercial LLMs for rapid prototyping\n• Implement robust evaluation frameworks\n• Focus on user feedback and metrics\n• Gradual scaling and optimization\n### Infrastructure Decisions\n\n• Careful evaluation of hosted vs. self-hosted options\n• Consideration of open-source alternatives\n• Focus on maintainable and scalable solutions\n• Balance between cost and performance\n### Quality Control\n\n• Implementation of automated evaluation systems\n• Continuous monitoring of output quality\n• Regular assessment of user satisfaction\n• Iterative improvement based on metrics\nThis case study from Discord provides valuable insights into the practical implementation of LLMs in production environments, highlighting the importance of systematic approach to development, deployment, and optimization of AI-powered features. Their framework emphasizes the balance between rapid development and robust production deployment, while maintaining focus on user experience, safety, and cost efficiency.\n\n\n",
         "2024-07-08/2024-07-14"
        ],
        [
         "4",
         "2024-07-31 13:30:00+00:00",
         "Optimizing Text-to-SQL Pipeline Using Agent Experiments",
         "Tech",
         "2024.0",
         "https://idinsight.github.io/tech-blog/blog/aam_pseudo_agent/",
         "idinsight",
         "question_answering,data_analysis",
         "fastapi,crewai,databases,scalability,reliability",
         "text to sql,llm,crewai,fastapi,rag,prompt engineering,agents,python,optimization,production deployment,evaluation",
         "rag,prompt_engineering,multi_agent_systems,model_optimization,cost_optimization,latency_optimization",
         "Ask-a-Metric developed a WhatsApp-based AI data analyst that converts natural language questions to SQL queries. They evolved from a simple sequential pipeline to testing an agent-based approach using CrewAI, ultimately creating a hybrid \"pseudo-agent\" pipeline that combined the best aspects of both approaches. While the agent-based system achieved high accuracy, its high costs and slow response times led to the development of an optimized pipeline that maintained accuracy while reducing query response time to under 15 seconds and costs to less than $0.02 per query.",
         "# IDInsight: Optimizing Text-to-SQL Pipeline Using Agent Experiments (2024)\n\nhttps://idinsight.github.io/tech-blog/blog/aam_pseudo_agent/\n\n## Short Summary\n\nAsk-a-Metric developed a WhatsApp-based AI data analyst that converts natural language questions to SQL queries. They evolved from a simple sequential pipeline to testing an agent-based approach using CrewAI, ultimately creating a hybrid \"pseudo-agent\" pipeline that combined the best aspects of both approaches. While the agent-based system achieved high accuracy, its high costs and slow response times led to the development of an optimized pipeline that maintained accuracy while reducing query response time to under 15 seconds and costs to less than $0.02 per query.\n\n## Long Summary\n\n# Ask-a-Metric: Evolution of a Production Text-to-SQL System\n\nAsk-a-Metric is a WhatsApp-based AI data analyst system that enables natural language querying of SQL databases, specifically designed for the development sector. The project showcases a practical evolution of LLM deployment approaches, moving from a simple pipeline to an agent-based system, and finally to an optimized hybrid solution.\n\n# Initial Simple Pipeline Implementation\n\n• Built using Python's FastAPI framework\n• Followed functional programming paradigm\n• Designed for rapid deployment and quick feedback\n• Encountered significant challenges:\n# Agent-Based Approach Exploration\n\n• Implemented using CrewAI framework\n• System Architecture:\n• Performance Metrics:\n• Target Requirements:\n# Optimization Insights from Agent Experiments\n\n• Key Observations:\n• Optimization Parameters:\n• Used agent experiments as implicit parameter space search\n# Final Pseudo-Agent Pipeline Implementation\n\n## Architecture Improvements\n\n• Object-oriented refactoring of codebase\n• Modular components:\n• Retained successful patterns from agent approach while eliminating overhead\n## Technical Optimizations\n\n• Task breakdown into smaller, focused steps\n• Removed unnecessary agent operations:\n• Data optimization:\n• Purpose-built tools with specific scopes\n## Performance Achievements\n\n• Response time reduced to < 15 seconds\n• Cost per query reduced to < $0.02\n• Maintained high accuracy levels from agent-based approach\n# Production Considerations and Future Development\n\n## Current Focus Areas\n\n• Continuous improvement of key metrics:\n## Feature Development Pipeline\n\n• Multi-turn chat capabilities\n• Streamlined user onboarding\n• Multi-language support\n• Context-specific adaptations\n## Deployment Strategy\n\n• Testing in multiple contexts\n• Pilot program feedback integration\n• Focus on accessibility for social impact sector\n• Emphasis on maintaining low costs while ensuring accuracy\n# Technical Lessons Learned\n\n• Agent-based systems can serve as excellent prototyping tools\n• Hybrid approaches can combine benefits of different architectures\n• Importance of modular design in LLM systems\n• Value of systematic optimization in production deployments\n• Balance between sophistication and practical constraints\nThe case study demonstrates a pragmatic approach to LLMOps, showing how different architectural approaches can be evaluated and combined to create an optimal production system. The evolution from a simple pipeline through agent-based experimentation to a hybrid solution provides valuable insights for similar LLM deployment projects.\n\n\n",
         "2024-07-29/2024-08-04"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>industry</th>\n",
       "      <th>year</th>\n",
       "      <th>source_url</th>\n",
       "      <th>company</th>\n",
       "      <th>application_tags</th>\n",
       "      <th>tools_tags</th>\n",
       "      <th>extra_tags</th>\n",
       "      <th>techniques_tags</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>full_summary</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-10 14:21:00+00:00</td>\n",
       "      <td>LLM Validation and Testing at Scale: GitLab's ...</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>https://about.gitlab.com/blog/2024/05/09/devel...</td>\n",
       "      <td>gitlab</td>\n",
       "      <td>code_generation,high_stakes_application,regula...</td>\n",
       "      <td>monitoring,cicd,devops,continuous_deployment,c...</td>\n",
       "      <td>llm,testing,evaluation,prompt engineering,metr...</td>\n",
       "      <td>prompt_engineering,error_handling,latency_opti...</td>\n",
       "      <td>GitLab developed a robust framework for valida...</td>\n",
       "      <td># Gitlab: LLM Validation and Testing at Scale:...</td>\n",
       "      <td>2024-07-08/2024-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-10 14:21:00+00:00</td>\n",
       "      <td>Building a Scalable Retriever-Ranker Architect...</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>https://blog.malt.engineering/super-powering-o...</td>\n",
       "      <td>malt</td>\n",
       "      <td>structured_output,realtime_application</td>\n",
       "      <td>kubernetes,monitoring,scaling,devops,orchestra...</td>\n",
       "      <td>recommenders,rerankers,vector database,qdrant,...</td>\n",
       "      <td>embeddings,semantic_search,vector_search,model...</td>\n",
       "      <td>Malt's implementation of a retriever-ranker ar...</td>\n",
       "      <td># Malt: Building a Scalable Retriever-Ranker A...</td>\n",
       "      <td>2024-07-08/2024-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-10 14:38:00+00:00</td>\n",
       "      <td>Building Secure and Private Enterprise LLM Inf...</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>https://slack.engineering/how-we-built-slack-a...</td>\n",
       "      <td>slack</td>\n",
       "      <td>regulatory_compliance,legacy_system_integration</td>\n",
       "      <td>security,compliance,guardrails,reliability,sca...</td>\n",
       "      <td>rag,aws,sagemaker,security,privacy,compliance,...</td>\n",
       "      <td>rag,semantic_search,error_handling,latency_opt...</td>\n",
       "      <td>Slack implemented AI features by developing a ...</td>\n",
       "      <td># Slack: Building Secure and Private Enterpris...</td>\n",
       "      <td>2024-07-08/2024-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-10 14:39:00+00:00</td>\n",
       "      <td>Building and Scaling LLM Applications at Discord</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>https://discord.com/blog/developing-rapidly-wi...</td>\n",
       "      <td>discord</td>\n",
       "      <td>chatbot,content_moderation,structured_output,r...</td>\n",
       "      <td>monitoring,scaling,devops,security,compliance,...</td>\n",
       "      <td>prompt engineering,evaluation,deployment,safet...</td>\n",
       "      <td>prompt_engineering,fine_tuning,model_optimizat...</td>\n",
       "      <td>Discord shares their comprehensive approach to...</td>\n",
       "      <td># Discord: Building and Scaling LLM Applicatio...</td>\n",
       "      <td>2024-07-08/2024-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-31 13:30:00+00:00</td>\n",
       "      <td>Optimizing Text-to-SQL Pipeline Using Agent Ex...</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>https://idinsight.github.io/tech-blog/blog/aam...</td>\n",
       "      <td>idinsight</td>\n",
       "      <td>question_answering,data_analysis</td>\n",
       "      <td>fastapi,crewai,databases,scalability,reliability</td>\n",
       "      <td>text to sql,llm,crewai,fastapi,rag,prompt engi...</td>\n",
       "      <td>rag,prompt_engineering,multi_agent_systems,mod...</td>\n",
       "      <td>Ask-a-Metric developed a WhatsApp-based AI dat...</td>\n",
       "      <td># IDInsight: Optimizing Text-to-SQL Pipeline U...</td>\n",
       "      <td>2024-07-29/2024-08-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  \\\n",
       "0  2024-07-10 14:21:00+00:00   \n",
       "1  2024-07-10 14:21:00+00:00   \n",
       "2  2024-07-10 14:38:00+00:00   \n",
       "3  2024-07-10 14:39:00+00:00   \n",
       "4  2024-07-31 13:30:00+00:00   \n",
       "\n",
       "                                               title industry    year  \\\n",
       "0  LLM Validation and Testing at Scale: GitLab's ...     Tech  2024.0   \n",
       "1  Building a Scalable Retriever-Ranker Architect...     Tech  2024.0   \n",
       "2  Building Secure and Private Enterprise LLM Inf...     Tech  2024.0   \n",
       "3   Building and Scaling LLM Applications at Discord     Tech  2024.0   \n",
       "4  Optimizing Text-to-SQL Pipeline Using Agent Ex...     Tech  2024.0   \n",
       "\n",
       "                                          source_url    company  \\\n",
       "0  https://about.gitlab.com/blog/2024/05/09/devel...     gitlab   \n",
       "1  https://blog.malt.engineering/super-powering-o...       malt   \n",
       "2  https://slack.engineering/how-we-built-slack-a...      slack   \n",
       "3  https://discord.com/blog/developing-rapidly-wi...    discord   \n",
       "4  https://idinsight.github.io/tech-blog/blog/aam...  idinsight   \n",
       "\n",
       "                                    application_tags  \\\n",
       "0  code_generation,high_stakes_application,regula...   \n",
       "1             structured_output,realtime_application   \n",
       "2    regulatory_compliance,legacy_system_integration   \n",
       "3  chatbot,content_moderation,structured_output,r...   \n",
       "4                   question_answering,data_analysis   \n",
       "\n",
       "                                          tools_tags  \\\n",
       "0  monitoring,cicd,devops,continuous_deployment,c...   \n",
       "1  kubernetes,monitoring,scaling,devops,orchestra...   \n",
       "2  security,compliance,guardrails,reliability,sca...   \n",
       "3  monitoring,scaling,devops,security,compliance,...   \n",
       "4   fastapi,crewai,databases,scalability,reliability   \n",
       "\n",
       "                                          extra_tags  \\\n",
       "0  llm,testing,evaluation,prompt engineering,metr...   \n",
       "1  recommenders,rerankers,vector database,qdrant,...   \n",
       "2  rag,aws,sagemaker,security,privacy,compliance,...   \n",
       "3  prompt engineering,evaluation,deployment,safet...   \n",
       "4  text to sql,llm,crewai,fastapi,rag,prompt engi...   \n",
       "\n",
       "                                     techniques_tags  \\\n",
       "0  prompt_engineering,error_handling,latency_opti...   \n",
       "1  embeddings,semantic_search,vector_search,model...   \n",
       "2  rag,semantic_search,error_handling,latency_opt...   \n",
       "3  prompt_engineering,fine_tuning,model_optimizat...   \n",
       "4  rag,prompt_engineering,multi_agent_systems,mod...   \n",
       "\n",
       "                                       short_summary  \\\n",
       "0  GitLab developed a robust framework for valida...   \n",
       "1  Malt's implementation of a retriever-ranker ar...   \n",
       "2  Slack implemented AI features by developing a ...   \n",
       "3  Discord shares their comprehensive approach to...   \n",
       "4  Ask-a-Metric developed a WhatsApp-based AI dat...   \n",
       "\n",
       "                                        full_summary             year_month  \n",
       "0  # Gitlab: LLM Validation and Testing at Scale:...  2024-07-08/2024-07-14  \n",
       "1  # Malt: Building a Scalable Retriever-Ranker A...  2024-07-08/2024-07-14  \n",
       "2  # Slack: Building Secure and Private Enterpris...  2024-07-08/2024-07-14  \n",
       "3  # Discord: Building and Scaling LLM Applicatio...  2024-07-08/2024-07-14  \n",
       "4  # IDInsight: Optimizing Text-to-SQL Pipeline U...  2024-07-29/2024-08-04  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv file into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"filtered_llmops_database.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b72442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, datetime, yaml, random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Load taxonomy ---\n",
    "TAX = yaml.safe_load(open(\"taxonomy.yaml\", \"r\"))\n",
    "CATALOG = TAX[\"categories\"]\n",
    "CAT_BY_KEY = {c[\"key\"]: c for c in CATALOG}\n",
    "ORDER = TAX[\"sections_order\"]\n",
    "\n",
    "# --- Model client wrapper (replace with your provider) ---\n",
    "# main.py\n",
    "import yaml, json\n",
    "from llm_openai import OpenAILLM\n",
    "from your_router_module import categorize_articles, to_markdown  # from the code I shared\n",
    "\n",
    "TAX = yaml.safe_load(open(\"taxonomy.yaml\"))\n",
    "CATALOG = TAX[\"categories\"]\n",
    "CATEGORY_KEYS = [c[\"key\"] for c in CATALOG]\n",
    "\n",
    "llm = OpenAILLM(model=\"gpt-4.1-mini\", temperature=0.3, seed=17)\n",
    "\n",
    "# Build prompts you already prepared:\n",
    "from your_router_module import SYSTEM_PROMPT, build_user_prompt\n",
    "\n",
    "def llm_adapter(article):\n",
    "    user_prompt = build_user_prompt(article)  # as previously defined\n",
    "    return llm.chat_json(SYSTEM_PROMPT, user_prompt, CATEGORY_KEYS)\n",
    "\n",
    "# Drop-in: replace the old LLM class usage with this adapter inside your pipeline.\n",
    "# Example single article:\n",
    "# result = llm_adapter(example_article)\n",
    "# print(result)\n",
    "\n",
    "# --- Helpers ---\n",
    "REPUTABLE = [\"arxiv.org\",\"nature.com\",\"engineering.\",\"research.\",\"openai.com\",\"deepmind.com\",\"nvidia.com\",\"techcrunch.com\",\"bloomberg.com\",\"ft.com\",\"wsj.com\"]\n",
    "\n",
    "def days_since(dt_iso: str) -> int:\n",
    "    try:\n",
    "        dt = datetime.datetime.fromisoformat(dt_iso.replace(\"Z\",\"+00:00\"))\n",
    "    except Exception:\n",
    "        return 30\n",
    "    now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    return max(0, (now - dt).days)\n",
    "\n",
    "def recency_boost(days:int)->float:\n",
    "    return max(0.0, min(1.0, 1.0/(1+0.05*max(days,1))))\n",
    "\n",
    "def authority_boost(url:str)->float:\n",
    "    u = (url or \"\").lower()\n",
    "    return 1.0 if any(tok in u for tok in REPUTABLE) else 0.6\n",
    "\n",
    "def novelty_boost(article:Dict[str,Any])->float:\n",
    "    tag_buckets = [article.get(\"application_tags\",\"\"), article.get(\"tools_tags\",\"\"),\n",
    "                   article.get(\"techniques_tags\",\"\"), article.get(\"extra_tags\",\"\")]\n",
    "    tags = set()\n",
    "    for b in tag_buckets:\n",
    "        for t in str(b).split(\",\"):\n",
    "            t=t.strip().lower()\n",
    "            if t: tags.add(t)\n",
    "    return min(1.0, 0.7 + 0.03*len(tags))\n",
    "\n",
    "def build_user_prompt(article:Dict[str,Any]) -> str:\n",
    "    return (\n",
    "        \"You will receive:\\n\"\n",
    "        \"1) Article fields (title, summaries, tags, url, company, created_at).\\n\"\n",
    "        \"2) Category catalog (key, definition, example_signals).\\n\\n\"\n",
    "        \"Tasks:\\n\"\n",
    "        \"A) Score every category from 0.0 to 1.0 (two decimals) based on evidence in the article.\\n\"\n",
    "        \"B) Pick the best category. Provide up to 2 secondaries if close.\\n\"\n",
    "        \"C) Give a short rationale citing which exact phrases/fields influenced the decision.\\n\"\n",
    "        \"D) Draft a crisp 1-sentence blurb for the newsletter (<=28 words).\\n\"\n",
    "        \"E) Compute a newsletter_score in [0,1] using: \"\n",
    "        \"0.45*category_score(best) + 0.25*recency_boost + 0.15*authority_boost + 0.15*novelty_boost.\\n\"\n",
    "        \"- recency_boost: 1/(1+0.05*days_since_created), clipped [0,1].\\n\"\n",
    "        \"- authority_boost: 1.0 for reputable domains (arxiv.org, nature.com, engineering.*.com, research.*.com, openai.com, nvidia.com, techcrunch.com, bloomberg.com, ft.com); else 0.6.\\n\"\n",
    "        \"- novelty_boost: 0.7 + 0.03*(unique_tags_count), clipped to 1.0.\\n\\n\"\n",
    "        \"JSON schema (return exactly this):\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"primary_category\": \"string\",\\n'\n",
    "        '  \"secondary_categories\": [\"string\", ...],\\n'\n",
    "        '  \"scores\": {\"category_key\": 0.00, \"...\": 0.00},\\n'\n",
    "        '  \"rationale\": \"string\",\\n'\n",
    "        '  \"blurb\": \"string\",\\n'\n",
    "        '  \"newsletter_score\": 0.00\\n'\n",
    "        \"}\\n\\n\"\n",
    "        \"Article:\\n\" + json.dumps(article, ensure_ascii=False) + \"\\n\\n\"\n",
    "        \"Categories:\\n\" + json.dumps(CATALOG, ensure_ascii=False)\n",
    "    )\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an editor routing tech news into newsletter sections. \"\n",
    "    \"Be decisive, concise, and consistent with the provided category definitions. \"\n",
    "    \"When unsure, still score all categories in [0.0, 1.0]. Favor the most specific category. \"\n",
    "    \"Return STRICT JSON that matches the JSON schema and nothing else.\"pro\n",
    ")\n",
    "\n",
    "def llm_route_article(llm: LLM, article: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    # Precompute boosts (the model uses these definitions; we also verify)\n",
    "    article = dict(article)  # shallow copy\n",
    "    article[\"_precomputed\"] = {\n",
    "        \"days_since\": days_since(article.get(\"created_at\",\"\")),\n",
    "        \"authority_boost\": authority_boost(article.get(\"source_url\",\"\")),\n",
    "        \"novelty_boost\": novelty_boost(article)\n",
    "    }\n",
    "    result = llm.chat_json(SYSTEM_PROMPT, build_user_prompt(article))\n",
    "    # Basic sanity checks\n",
    "    if \"primary_category\" not in result or \"scores\" not in result:\n",
    "        raise ValueError(\"Model returned invalid JSON.\")\n",
    "    return result\n",
    "\n",
    "def self_consistent_route(llm: LLM, article: Dict[str,Any], votes:int=3) -> Dict[str,Any]:\n",
    "    \"\"\"Run N votes and merge by majority + score average.\"\"\"\n",
    "    tallies = {}\n",
    "    seconds = {}\n",
    "    rationales = []\n",
    "    blurbs = []\n",
    "    scores_agg = {}\n",
    "\n",
    "    for i in range(votes):\n",
    "        out = llm_route_article(llm, article)\n",
    "        pc = out[\"primary_category\"]\n",
    "        tallies[pc] = tallies.get(pc,0)+1\n",
    "        for s in out.get(\"secondary_categories\",[]):\n",
    "            seconds[s] = seconds.get(s,0)+1\n",
    "        # aggregate per-category scores\n",
    "        for k,v in out[\"scores\"].items():\n",
    "            scores_agg.setdefault(k, []).append(float(v))\n",
    "        rationales.append(out.get(\"rationale\",\"\"))\n",
    "        blurbs.append(out.get(\"blurb\",\"\"))\n",
    "    # pick primary by majority then by highest mean score\n",
    "    primary = max(tallies.items(), key=lambda kv: (kv[1], sum(scores_agg.get(kv[0],[0]))/max(1,len(scores_agg.get(kv[0],[])))))[0]\n",
    "    # compute averaged per-category scores\n",
    "    mean_scores = {k: round(sum(v)/len(v), 2) for k,v in scores_agg.items()}\n",
    "    # secondaries: top two aside from primary\n",
    "    secondaries = [k for k,_ in sorted(((k,v) for k,v in mean_scores.items() if k!=primary), key=lambda kv: kv[1], reverse=True)[:2]]\n",
    "    # pick most frequent blurb and a concise rationale\n",
    "    blurb = max(set(blurbs), key=blurbs.count)\n",
    "    rationale = rationales[0][:200]\n",
    "    # recompute newsletter_score deterministically client-side as a guard\n",
    "    days = days_since(article.get(\"created_at\",\"\"))\n",
    "    nscore = (\n",
    "        0.45*mean_scores.get(primary,0.0) +\n",
    "        0.25*recency_boost(days) +\n",
    "        0.15*authority_boost(article.get(\"source_url\",\"\")) +\n",
    "        0.15*novelty_boost(article)\n",
    "    )\n",
    "    return {\n",
    "        \"primary_category\": primary,\n",
    "        \"secondary_categories\": secondaries,\n",
    "        \"scores\": mean_scores,\n",
    "        \"rationale\": rationale,\n",
    "        \"blurb\": blurb,\n",
    "        \"newsletter_score\": round(min(1.0, max(0.0, nscore)), 3)\n",
    "    }\n",
    "\n",
    "# --- End-to-end ---\n",
    "def categorize_articles(articles: List[Dict[str,Any]], llm: LLM, votes=3) -> List[Dict[str,Any]]:\n",
    "    out = []\n",
    "    for a in articles:\n",
    "        res = self_consistent_route(llm, a, votes=votes)\n",
    "        out.append({**a, **res})\n",
    "    return out\n",
    "\n",
    "def group_for_newsletter(labeled: List[Dict[str,Any]]) -> Dict[str, List[Dict[str,Any]]]:\n",
    "    buckets = {k: [] for k in ORDER}\n",
    "    for item in labeled:\n",
    "        k = item[\"primary_category\"]\n",
    "        if k not in buckets:  # unseen category fallback\n",
    "            buckets.setdefault(\"opinion_analysis\", []).append(item)\n",
    "        else:\n",
    "            buckets[k].append(item)\n",
    "    # sort within sections\n",
    "    for k in buckets:\n",
    "        buckets[k].sort(key=lambda x: (x[\"newsletter_score\"], x.get(\"created_at\",\"\")), reverse=True)\n",
    "    return buckets\n",
    "\n",
    "def to_markdown(buckets: Dict[str,List[Dict[str,Any]]]) -> str:\n",
    "    lines = []\n",
    "    for k in ORDER:\n",
    "        items = buckets.get(k, [])\n",
    "        if not items: continue\n",
    "        lines.append(f\"## {CAT_BY_KEY[k]['name']}\")\n",
    "        for it in items:\n",
    "            date = it.get(\"created_at\",\"\")[:10]\n",
    "            title = it.get(\"title\",\"\").strip()\n",
    "            url = it.get(\"source_url\",\"\")\n",
    "            blurb = it.get(\"blurb\",\"\")\n",
    "            lines.append(f\"- [{title}]({url}) • {date} — {blurb}\")\n",
    "        lines.append(\"\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8db79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
